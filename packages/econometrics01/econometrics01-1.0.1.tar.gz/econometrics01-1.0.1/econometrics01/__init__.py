import pyperclip as pc

tasks = {
    '1. Предмет и метод эконометрики':
        '''Предмет - причинно-следственные связи между социально-экономическими явлениями или процессами 
 Метод - исследование взаимосвязей из количественного описания через переменные Y и X (построение экономических моделей)
''',

    '2. Измерения в эконометрике. Уровни и типы шкал измерения в экономике':
        '''1. Низкий \
    a. сравнение объектов по наличию/отсутствию свойств
     пример: номинация, классификация, нумерация
     Номинальная шкала - инн, пол людей, номера студ билетов)
    2. Средний \
    a. сравнение объектов по интенсивности проявления свойств
     пример: упорядочение, ротирование, топология
    шкала ЕГЭ, шкала оценок, шкала силы землетрясений 
    3. Высший\
    a. сравнение объектов с эталоном (единица измерения)
     пример: шкала цельсия, измерение массы 
    ''',

    '3. Этапы эконометрического моделирования':
        '''Этапы эконом моделирования:
    1. Постановка цели исследования
    2. Сбор данных и первичная обработка; Анализ требований;
    3. Спецификация модели - выбор функциональной формы модели и состава переменных входящих в неё
    4. параметризация модели
    a. метод наименьших квадратов (МНК)
    b.  метод максимального правдоподобия (ММП)
    c.  метод момента
    5. Верификация модели ( проверка статистическая гипотез)
    6. Интерпретация результатов моделирования и разработка рекомендаций по улучшению её качества
    ''',

    '4. Виды эконометрических моделей и типы переменных в модели':
        '''1. Регрессионные модели 
    2. Динамическая модель
    3. Системы эконометрических уравнений
    
    1. Зависимые (y), результативная; Независимые (x), факторные, экзогенные, факторные
    2. количественные - (дискретные и непрерывные); качественные
    3. Xt, Yt - значение временни; лаговые
    ''',

    '5. Корреляционный анализ взаимосвязи: Парная корреляция Оценки значимости. Интервальные оценки для значимых параметров связи':
        r'''Корреляционный анализ используется для изучения взаимосвязи между признаками. Для измерения степени этой взаимосвязи используется коэффициент корреляции.
    Парный коэффициент корреляции измеряет степень тесноты взаимосвязи между двумя переменными. Обозначается как r и принимает значения [-1;1]. r>0 связь прямая, r<0 связь обратная. Степень тесноты опред. По шкале Чеддока: 0 – связь отстутв., 0-0,3 очень слабая, 0,3-0,5 слабая, 0,5-0,7 умеренная, 0,7-0,9 высокая, 1 функциональная.
    Чтобы оценить значимость коэф. корр. используется тест Стьюдента. Если t набл > t крит, то коэф. корр. признается значимым.\
    $t_{расч} = |r_{xy}| \sqrt{\dfrac{n-2}{1-r^2_{xy}}}$\
    Интервальные оценки могут использоваться для получения доверительных интервалов вокруг коэффициента корреляции. Это позволяет оценить диапазон значений, в пределах которого с определенной вероятностью находится истинное значение коэффициента корреляции.''',

    '6. Корреляционный анализ взаимосвязи: Индекс корреляции. Оценка значимости. Интервальные оценки для значимых параметров связи':
        r'''Мерой тесноты нелинейной корреляционной связи является индекс корреляции, определяемый из соотношения\
        $R = \sqrt{1 - \dfrac{\Sigma{(y- \hat y)^2}}{\Sigma{(y- \bar y)^2} }}$\
        где $\hat y$ – расчетные значения зависимой переменной по нелинейной регрессии.
    Величина индекса корреляции находится в пределах от нуля до единицы, и чем ближе его значение к 1, тем теснее рассматриваемая связь.
    Значимость индекса корреляции проверяется так же, как и коэффициента корреляции.\
    $t_{расч} = |r_{xy}| \sqrt{\dfrac{n-2}{1-r^2_{xy}}}$\
    Интервальные оценки могут использоваться для получения доверительных интервалов вокруг коэффициента корреляции. Это позволяет оценить диапазон значений, в пределах которого с определенной вероятностью находится истинное значение коэффициента корреляции.''',

    '7. Корреляционный анализ взаимосвязи качественных признаков: Индекс корреляции Оценка значимости. Интервальные оценки для значимых параметров связи':
        r'''Индекс корреляции - нормированный показатель тесноты связи. 
    - Для линейной связи его значение равно коэффициенту корреляции.
    - Для нелинейных регрессионных моделей (можно использовать и для линейной):\
    $R = \sqrt{1 - \dfrac{\Sigma{(y- \hat y)^2}}{\Sigma{(y- \bar y)^2} }}$\
    где у - настоящее значение У, $\hat y$ - предсказанное значение У, $\bar y$ - среднее значение У
    Измеряется от 0 до 1, не показывает направление, а только силу корреляции
    Индекс множественной корреляции также можно найти с помощью стандартизованных коэффициентов регрессии B_j :
    $R_{yx_1...x_p} = \sqrt{ \Sigma{\beta_j r_{yx_j}} }$
    где B j – стандартизованный коэффициент регрессии при переменной  t_xj 
     r_yx_j – парный коэффициент корреляции между зависимой переменной Y и соответствующей объясняющей переменной X_j.
     77777
    Оценка значимости:
    Проверка статистической значимости результатов множественной регрессии производится по аналогии с парной, однако в данном случае удобнее пользоваться матричной записью. 
    1 этап. Для проверки значимости уравнения регрессии следует рассчитать показатели таблицы дисперсионного анализа. Факторная, остаточная и общая вариации будут рассчитываться по формулам:
    Затем на их основе получают значения факторной $S^2_r$ и остаточной $S^2_e$ дисперсий:\
    $S^2_r = \dfrac{Q_r}{p}$\
    $S^2_e = \dfrac{Q_e}{n-p-1}$\
    n - количество наблюдений\
    p - количество регрессоров х
    
    и расчетное значение F-критерия Фишера-Снедекора:\
    $F_p = \dfrac{S^2_r}{S^2_e}$\
    После чего, сравнивая значения $F_p$ и $F_{a;k1;k2}$ принимают  или отвергают гипотезу $H_0 S^2_r = S^2_e$
    Уравнение множественной регрессии будет значимо в том случае, если $F_p$ > $F_{a;k1;k2}$ где $F_{a;k1;k2}$ табличное значение F-критерия, определяемое по таблице F-распределения на уровне значимости
    a = 0.05 для параметров k_1 = p (число степеней свободы факторной дисперсии), k_2 = n-p-1 (число степеней свободы остаточной дисперсии)
    77777
    В процессе подбора оптимального состава объясняющих переменных нужно помнить, что при включении существенного фактора в модель величина факторной дисперсии $S^2_r$ должна увеличиваться, а величина остаточной дисперсии $S^2_e$ - уменьшаться.
    
    2 этап - Для проверки значимости параметров уравнения регрессии сначала необходимо рассчитать их стандартные ошибки:
    Затем получить расчетные значения t-статистик как отношение параметра b_j к его стандартной ошибке m_b_i
    
    Параметр b_j явл. статистически значимым если t > t табличного на уровне значимости a = 0.05, df=n-p-1
    если гипотиза подтверждается, то коэф b_j признается статистически незначимым и соответствующая ему переменная X_j исключается из анализа\
    Если гипотеза отвергается, то коэф b_j признается статистически значимым, а соответствующую ему переменную X_j из анализа не исключают
    
    Интервальные оценки для значимых параметров связи:\
    Статистическую значимость параметров такде можно проверить с помощью их интервальной оценки. Здесь для каждого параметра рассчитывают доверительные интервалы\
    $b_j - m_{b_j} t_{a;df} < = b_j < = b_j + m_{b_j} t_{a;df}$\
    Если для какого-либо b_j интервал проходит через точку ноль, параметр признается статистически незначимым, а соответствующая
    объясняющая переменная исключается из анализа''',

    '8. Канонические корреляции и канонические величины: оценка и интерпретация':
        r'''Метод канонических корреляций является обобщением парной корреляции и позволяет находить
    максимальные корреляционные связи между двумя груплами случайных величин. Эта зависимость.
    определяется при помощи новых аргументов - канонических величин (канонических переменньх),
    вычисленных как линейные комбинации исходных признаков по каждой из групп. Эти канонические
    величины должны максимально коррелировать между собой, а их число определяется по числу
    переменных в меньшем множестве (если число переменных в них не одинаково).
    
    Например, эффективность работы предприятий оценивается такими показателями как
    производительность труда, фондоотдача основных фондов, прибыль, рентабельность и другими.
    Факторами, влияющими на показатели, являются численность работающих, стоимость основных
    Фондов, оборачиваемость оборотных средств, удельный вес потерь от брака, трудоемкость единицы
    продукции, коэффициент сменности работы оборудования и тому подобные. Метод канонических
    корреляций позволяет анализировать взаимосвязь нескольких выходных показателей и большого
    числа определяющих факторов.
    77777
    Пусть имеется k-мерный случайный вектор Х. Не умаляя общности можем предположить, что.
    математическое ожидание вектора равно нулю, дисперсии компонент равны единице,
    корреляционная матрица В положительно определена. Вектор Х разбивается на два подвектора Х, и
    Х» размерности пи р соответственно. При этом m+р=k и m<=p. Подвекторы образуют две группы
    показателей. Задача заключается в выявлении максимальных связей между этими группами. Для этого вводят новые переменные (канонические переменные) d_11 и d_12 \
    d_11 = a_1*x_11 + a_2*x_21 + ... + a_i*x_i1 + ... + a_m*x_m1 \
    d_12 = b_1*x_12 + b_2*x_22 + ... + b_j*x_j2 + ... + b_p*x_p2 \
    
    где d_11 и d_12 - первая пара канонических переменных\
    x_i1 - i-тая компонента подвектора Х_1; x_j2 - j-тая компонента подвектора Х_2; a_i и b_j - коэффициенты \
    i= 1,...,m; j= 1,...,p
    
    Корреляция между d_11 и d_12 должна быть максимальной среди всех других возможных линейных комбинаций (канонических переменных). Далее в каждой группе рассматриваются следующие линейные комбинации d_21 и d_22, у которых корреляция больше чем между любыми другими линейными комбинациями, не коррелированными с первыми линейными комбинациями. Затем по аналогии пары d_31 и d_32, d_41 и d_42 и тд. В общем случае должно быть m корреляций между каноническими переменными, которые не коррелируют с другими.
    Общая корреляционная матрица X^T * X вектора Х может быть представлена совокупностью подматриц.
    
    Далее определяется матрица В размером m*m 
    
    $B = R^{-1}_{11}*R_{12}*R^{-1}_{22}*R_{21}$
    
    Собственные значения этой матрица, ранжированные по убыванию, равняются квадратам коэффициентов канонических корреляций. Для разрешимости задачи необходимо, чтобы корреляционные матрицы $R_{11}$ и $R_{22}$ были положительно определены. Это означает, что в составе Х_1 и Х_2 не должны существовать линейно зависимые компоненты. В противном случае следует один или несколько показателей-факторов исключить.
    77777
    Канонические переменные обладают следующими свойствами:
    1) Канонические переменные одной группы взаимно не коррелированны.
    2) Канонические переменные одной группы взаимно не коррелированны
    3) Канонические переменные выбраны таким образом, чтобы соответствующие канонические корреляции были максимальны
    4) канонические переменные упорядочены по мере убывания соответствующих канонических корреляций
    5) число используемых канонических корреляций не превосходит число исследуемых показателей m <= p
    
    Канонические корреляции всегда неотрицательны. Чем больше значения канонических корреляций, тем сильнее связаны группы признаков X_1 и X_2 показателей.
    
    Значимость канонических корреляций проверяется с использованием критерия Пирсона X^2. Если вычеслено m канонических корреляций p1, p2,...,p1m то необходимо прроверить m нулевых гипотез: \
    $H_{0j} : p_j=0, j=1,...,m$ \
    При этом, необходимо учитывать, что канонические корреляции упорядочены по величине p_m > p_m-1>...>p_1
    При проверке гипотез статистика Х^2 вычисляется по формуле \
    $X^2_j = [n-j-0.5 * (k+1) + \Sigma{r_f ^2}] * \ln{(\Pi(1-r_|^2)})$
    
    где n - объем выборки, к - размерность вектора Х=(Х_1, Х_2)^T; r_f - оценка f-го коэфы канонической корреляции (f=1,...,j-1; r_| - оценка |-го коэфа канонической корреляции (f=1,..,m); число степеней свободы статистики v = [(p-j+1)*(m-j+1)]
    
    Если значение статистики превосходит критическое значение при заданном уровне значимости а, или p-value не превосходит a, то данные противоречат гипотезе и p_j отлично от нуля. Так как значения канонических корреляций упорядочены, при p_j=0, то и остальные m - j значений канонических корреляций равны нулю''',

    '9. Парная линейная регрессия, спецификация модели и экономическая интерпретация ее параметров':
        r'''Парная линейная регрессия – это уравнение парной регрессии, состоящее из одной зависимой и одной независимой переменных, которое можно представить в виде прямой
    Теоретическое уравнение: \
    $y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$
    
    где $\beta_0$ и $\beta_1$ коэфы регрессии\
    у - результативный признак \
    х - факторный признак \
    $ \varepsilon_i$ - случайный фактор 
    77777
    Эмпирическое уравнение \
    $y_i = b_0 + b_1 x_i + e_i$
    
    где b_0 и b_1 - оценки коэфов регрессии  $\beta_0$ и $\beta_1$ \
    $e_i$ - оценка случайного фактора $\varepsilon_i$
    
    Спецификация модели: выбираются у и х, выбирается форма уравнения регрессии \
    экономическая интерпретация параметров:\
    полученные оценки $b_0, b_1$ называются коэфами "чистой" регрессии
    параметр b_1 показывает на сколько единиц измерения (килограммов, тонн, рублей, долларов и т.д.) в среднем изменится переменная y при изменении x на одну единицу измерения.''',

    '10. Парная нелинейная регрессия. Нелинейная относительно включенных в анализ переменных, но линейная по оцениваемым параметрам. Примеры экономических моделей':
        r'''Парная нелинейная регрессия относится к моделям, где зависимость между зависимой переменной и одной независимой переменной является нелинейной. \
    Например: $y = e^{b_0 + b_1x +\varepsilon_i}$ 
    
    Примером нелинейной регрессии по включаемым в нее объясняющим переменным могут служить следующие функции:
    1) полиномы разных степеней:  $y = b_0 + b_1 x^2 + \varepsilon_i$
    2) равносторонняя гипербола: $y = b_0 + \dfrac{b_1}{x} + \varepsilon_i$
    
    Нелинейная регрессия по включенным переменным определяется, как и в линейной регрессии, методом наименьших квадратов (МНК), ибо эти функции линейны по параметрам, только проводится замена (линеаризация???) переменных
    77777
    например в уравнении $y = b_0 + b_1 x^5 + \varepsilon_i$; $x^5$ заменяется на $x^*$ и мы получаем уравнение \
    $y = b_0 + b_1 x^* + \varepsilon_i$
    
    примеры экономических моделей:\
    модель филлипса: $y = b_0 + \dfrac{b_1}{x} + \varepsilon_i$ \
    где у - прирост заработной платы, ч - уровень безработицы''',

    '11. Парная нелинейная регрессия. Нелинейная по оцениваемым параметрам. Примеры экономических моделей':
        r'''Парная нелинейная регрессия относится к моделям, где зависимость между зависимой переменной и одной независимой переменной является нелинейной.
    К данному классу регрессий относятся уравнения, в которых зависимая переменная нелинейно связана с параметрами. Примером таких нелинейных регрессий являются функции:
    1) степенная          $y=b_0x^{b_1}\varepsilon_i$
    2) показательная      $y=b_0b_1^x\varepsilon_i$
    3) экспоненциальная   $y=e^{b_0+b_1x+\varepsilon_i}$
    4) обратная функция   $y_i = \dfrac{1}{\beta_0+\beta_1x_i+\varepsilon_i}$
    Примеры экономических моделей:
    Примером нелинейной по параметрам регрессии внутренне линейной является степенная функция, которая широко используется в эконометрических исследованиях при изучении спроса от цен: y=ax^b ε, где у — спрашиваемое количество; х — цена
    Данная модель нелинейна относительно оцениваемых параметров.
    ''',

    '12. Методы оценивания параметров регрессионной модели: метод наименьших квадратов':
        r'''Метод наименьших квадратов — это метод оценки параметров регрессионной модели, основанный на минимизации суммы квадратов остатков (остаток — это разница между наблюдаемым значением и подогнанным значением, предоставленным моделью), полученных в результате каждого отдельного уравнение.
    
    Найти аппроксимирующую функцию y = f(x,a,b,..)   (1)
    
    Для отыскания коэфов a, b,.. в функции (1) применяется МНК, который состоит в следующем. Между искомой функцией и табличными щначениями в точках $x_i$ наблюдаются отклонения. \
    Обозначим их $\Delta y_i = f(x_i,a,b,..) - y_i$, где i = 1,2,3..n \
    Выбираем значения коэфов a, b,.. так, чтобы сумма квадратов отклонений принимала минимальное значение \
    $S(a,b,..) = \sum_{i=1}^n (\Delta y_i)^2 = \sum_{i=1}^n [f(x_i,a,b,..)-y_i]^2 \rightarrow min$ (2)
    77777
    Сумма S(a,b,..) является функцией нескольких переменных. необходимый признак экстремума функции нескольких переменных состоит в том, что обращаются в ноль частные производные \
    $S'_a=0$, $S'_b=0$, ... (3)
    
    План решения задачи:
    1) выбираем функцию у=f(x,a,b,..)
    2) Для отыскания коэфов a,b,.. составляем систему уравнений (3)
    3) Решая систему уравнений (3), находим значения коэфов a,b,..
    4) Подставляя a,b,.. в уравнение (1), получаем искомую функцию y = f(x,a,b,..)''',

    '13. Методы оценивания параметров регрессионной модели: метод максимального правдоподобия':
        r'''Известен закон распределения случайных величин, зависящий от набора параметров. Оценки этих параметров подбираются так, чтобы вероятность получить имеющийся набор данных была максимальной. 
    
    $L(\theta) = P(X_1 = x_1, ... , X_n = x_n|\theta) \rightarrow max$ - для дискретных случайных величин
    
    $L(\theta) = f(X_1 = x_1, ... , X_n = x_n|\theta) \rightarrow max$ - для непрерывных случайных величин''',

    '14. Методы оценивания параметров регрессионной модели: метод моментов':
        r'''Суть метода моментов заключается в вычислении того количества теоретических и выборочных моментов случайной величины, которое равно числу исследуемых параметров регрессионной модели. После вычисления соответствующие друг другу теоретические и выборочные моменты приравниваются, и исходя из получившегося уравнения осуществляется вычисление оценки параметра.
    
    Пусть $X = (x_1,x_2,..,x_n)$ - независимая выборка из распределения $F_0$, зависящего от неизвестного параметра $\theta=(\theta_1,\theta_2,..,\theta_k)\in\Theta\subset R^k$ Моментом i-го порядка называется функция \
    $\mu(\theta_1,..,\theta_k=E[x^i] =$ система
    1) $\int x^i f(x,\theta_1,..,\theta_k)dx$, если х - непрерывная величина
    2) $\sum_i x^i_j p(x_j,\theta_1,..,\theta_k)$, если х - дискретная величина
    
    где $f(x,\theta)$ - плотность распределения непрерывной случайной величины х, $p(x_j,\theta)$ - вероятность дискретной случайной величины. Теоретический момент является функцией неизвестных параметров $\mu_i(\theta)=\mu_i(\theta_1,\theta_2,..,\theta_k)$
    
    Выборочным (эмпирическим) моментом i-го порядка называется величина \
    $m_i(x_1,x_2,..,x_n)=\dfrac{1}{n}\sum_{j=1}^nx^i_j$
    77777
    Отметим, что по своему определению эмпирические моменты являются функциями от выборки. \
    Для нахождения неизвестных параметров (будем обозначать их $\theta=(\theta_1,..,\theta_k)$, составим систему уравнений \
    $\mu_1(\theta_1,..,\theta_k)=m_1$,\
    ...\
    $\mu_k(\theta_1,..,\theta_k)=m_k$
    
    Далее решаем систему относительно параметров $\theta=(\theta_1,..,\theta_k)$. В результате получим \
    $\theta_1=\theta_1(x_1,..,x_n)$,\
    ... \
    $\theta_k=\theta_k(x_1,..,x_n)$
    
    Найденные параметры зависят от выборки $x = (x_1,..,x_n)$''',

    '15. Проверка значимости коэффициентов парной регрессии':
        r'''Значимость коэфа $b_0$
    Выдвигаются гипотизы: \
    $H_0:b_0=0$\
    $H_1:b_0\neq0$\
    
    Для проверки нулевой гипотещы используется t-распределение Стьюдента\
    $t_{расч} = \dfrac{b_0}{S_{b_0}}$\
    $S_{b_0} = \sqrt{ s^2 \dfrac{ \sum x^2_i }{n*\sum (x_i - \bar x)^2} }$ \
    $s^2 = \dfrac{ \sum (y_i - \hat y)^2 }{n-2}$
    77777
    t табличное (a, k) находится из таблицы \
    где a = 0.05, если не задано\
    k - число степеней свободы (k = n - 2)
    
    Вывод: если $|t_{расч}| > t_{табл}$, то нулевая гипотиза отвергается и $b_0$ статистически значимо
    
    
    Значимость коэфа $b_1$ 
    
    $t_{расч} = \dfrac{b_1}{S_{b_1}}$\
    $S_{b_0} = \sqrt{ s^2 \dfrac{1}{\sum (x_i - \bar x)^2} }$ \
    $s^2 = \dfrac{ \sum (y_i - \hat y)^2 }{n-2}$
    
    Вывод: если $|t_{расч}| > t_{табл}$, то нулевая гипотиза отвергается и $b_0$ статистически значимо''',

    '16. Проверка значимости уравнения парной регрессии':
        r'''Правило сложения дисперсий:\
    $Var_{общ} = Var_{факторная} + Var_{остаточная}$
    
    Var общая - характеризует изменения результативного признака за счет изменения всех факторов \
    Var факторная - характеризует изменения результативного признака за счет изменения учтенного фактора \
    Var остаточная - характеризует изменения результативного признака за счет изменения неучтенных факторов
    
    Выдвигаются гипотизы:\ 
    $H_0:Var_{факторная}=Var_{остаточная}$\
    $H_1:Var_{факторная}\neq Var_{остаточная}$ 
    77777
    Нулевая гипотеза проверяется с помощью F-статистики Фишера
    
    $F_{расч} = \dfrac{ r^2_{xy} }{ 1 - r^2_{xy} } (n - 2)$ \
    F табл (a, k1, k2) находится из таблицы, \
    где а = 0.05, если не задано\
    k1 = p (количество регрессоров х)\
    k2 = n - p - 1 (число степеней свободы)
    
    Вывод: если |Fрасч| > |Fтабл|, то нулевая гипотеза отвергается $\rightarrow$ уравнение парной регрессии можно считать статистически значимым 
    ''',

    '17. Оценка доверительного прогноза на основе парного уравнения регрессии':
        r''' $\sigma_{y_0}=\sigma_u\sqrt{ 1+\dfrac{1}{n}+\dfrac{ (x_0-\bar x)^2 }{n * Var(x)} }$ \
    $\sigma_u = \sqrt{ \dfrac{ \Sigma(y-\hat y)^2 }{ n-2 } }$ \
    
    или для парной регрессии можно использовать более понятную формулу\
    $\sigma_{y_0}=\sigma_u\sqrt{ 1+\dfrac{1}{n}+\dfrac{ (x_0-\bar x)^2 }{ \sum_{i=1}^n (x_i - \bar x)^2 } }$ 
    
    Доверительный интервал прогноза\
    $y_0 - t_{крит} * \sigma_{y_0} : y_0 + t_{крит} * \sigma_{y_0}$''',

    '18. Множественная линейная регрессия, спецификация модели и экономическая интерпретация ее параметров':
        r'''Множественная линейная регрессия - модель, где среднее значение переменной y является функцией нескольких переменных x1, x2, …, xm \
    $y = f(x_1,..,x_m) + \varepsilon$
    
    y - эндогенная (зависимая) переменная, результативный признак \
    x1, x2, …, xm - экзогенные переменные (факторы) \
    $\varepsilon$ - погрешность
    77777
    теоретическое уравнение: \
    $y = \beta_0 + \beta_1x_1 + ... + \beta_mx_m + \varepsilon$
    
    эмпирическое: \
    $\hat y = b_0 + b_1x_1 + ... + b_mx_m + \varepsilon$
    
    в матричном виде: \
    Y = XB + E
    
    экономическая интерпретация параметров:
    
    Полученные оценки $b_0, b_1, …, b_m$ называются коэффициентами «чистой» регрессии. 
    
    Параметр $b_j$ показывает, на сколько единиц измерения (килограммов, тонн, рублей, долларов и т.д.) в среднем изменится переменная Y при изменении j - й объясняющей переменной $x_j$ на одну единицу измерения. 
    ''',

    '19. Множественная нелинейная регрессия. Нелинейная относительно включенных в анализ переменных, но линейная по оцениваемым параметрам. Примеры экономических моделей':
        r'''В случае, когда несколько явлений могут быть соединены между собой нелинейными соотношениями, для описания зависимостей следует воспользоваться множественной нелинейной регрессией. \
    Примером нелинейной регрессии относительно включенных в анализ переменных, но линейной по оцениваемым параметрам могут служить следующие функции:
    1) полиномы разных степеней: \
    $y = a +bx+cx^2 + \varepsilon$ \
    $y = a +bx+cx^2 + dx^3 + \varepsilon$ 
    2) равносторонняя гипербола: $y = a + \dfrac{b}{x}+\varepsilon$
    
    Нелинейная регрессия по включенным переменным определяется, как и в линейной регрессии, методом наименьших квадратов (МНК), ибо эти функции линейны по параметрам. 
    Пример экономической модели:
    Также классическим ее примером является кривая Филлипса, характеризующая нелинейное соотношение между нормой безработицы 
    х и процентом прироста заработной платы у.
    ''',

    '20. Множественная нелинейная регрессия. Нелинейная по оцениваемым параметрам. Примеры экономических моделей':
        r'''В случае, когда несколько явлений могут быть соединены между собой нелинейными соотношениями, для описания зависимостей следует воспользоваться множественной нелинейной регрессией. 
    
    Нелинейными по оцениваемым параметрам моделями регрессии называются модели, в которых результативная переменная $y_i$ нелинейно зависит от коэффициентов модели 0…n.
    
    К моделям регрессии, нелинейными по оцениваемым параметрам, относятся:
    1) степенная функция: $y_i = \beta_0 * x^{\beta_1} * \varepsilon_i$
    2) показательная функция: $y=\beta_0*\beta_1^{x_i}*\varepsilon_i$
    3) логарифмическая парабола: $y_i = \beta_0 * \beta_1^{x_i}*\beta^{x^2_i}*\varepsilon_i$
    4) экспоненциальная функция: $y=e^{b_0+b_1x}*\varepsilon_i$
    5) обратная функция   $y_i = \dfrac{1}{\beta_0+\beta_1x_i+\varepsilon_i}$
    <br><br>
    6) кривая Гомперца: $y_i = k * \beta_0^{\beta_1^x}$
    7) логистическая функция или кривая Перла-Рида: (написать что забыл формулу)
    
    Пример экономической модели:
    
    Примером нелинейной по параметрам регрессии внутренне линейной является степенная функция, которая широко используется в эконометрических исследованиях при изучении спроса от цен:  , где $у=a*x^b$ — спрашиваемое количество; х — цена
    Данная модель нелинейна относительно оцениваемых параметров, 
    т. к. включает параметры а и b неаддитивно. 
    ''',

    '21. Множественная корреляция. Индекс детерминации':
        r'''Практическая значимость уравнения множественной регрессии оценивается 
    с помощью показателя множественной корреляции и его квадрата – коэффициента детерминации.
    
    Показатель множественной корреляции характеризует тесноту связи рассматриваемого набора факторов с исследуемым признаком, или, иначе, оценивает тесноту совместного влияния факторов на результат.\
    Независимо от формы связи показатель множественной корреляции может быть найден как индекс множественной корреляции:\
    $R_{yx_1,..,x_p} = \sqrt{1-\dfrac{S^2_{ост}}{S^2_y}}$\
    где $S^2_y$ - общая дисперсия результативного признака\
    $S^2_{ост}$ - остаточная дисперсия для уравнения $y = f(x_1,..,x_p)$
    77777
    Коэф $R^2$ показывает долю объясненной вариации зависимой переменной:\
    $R^2 = 1 - \dfrac{ \Sigma e^2_i }{ \Sigma (y_i - \bar y)^2 }$
    
    Используется для предварительной оценки качества модели и как основа для расчета других показателей\
    Коэф $R^2$ в разных моделях с разным числом наблюдений (и переменных) несравнимы''',

    '22. Выбор наилучшей формы модели (Спецификация модели регрессии)':
        r'''Одним из базовых предположений построения качественной модели является правильная (хорошая) спецификация уравнения регрессии. Правильная спецификация уравнения регрессии означает, что оно в целом верно отражает соотношение между изучаемой переменной и объясняющими факторами, участвующими в модели. Это является необходимой предпосылкой дальнейшего качественного оценивания регрессионной модели.
    
    Принципы спецификации эконометрических моделей:
    1. Спецификация модели возникает в результате трансляции на математический язык взаимосвязей исходных данных экономической задачи и ее искомых неизвестных.
    2. Количество уравнений, составляющих спецификацию модели, в точности совпадает с количеством эндогенных переменных, включенных в модель.
    3. Учет фактора времени или датирование переменных.
    4. Включение в модель случайных возмущений (остатков, ошибок).
    77777
    Типы переменных:
    1) Экзогенные переменные – это исходные данные задачи, определяются вне модели.
    2) Эндогенные переменные - это переменные, значения которых определяются при помощи модели (т.е. искомые неизвестные).
    3) Переменные модели называются датированными, если обозначена их зависимость от времени.
    4) Лаговыми называются экзогенные или эндогенные переменные, датированные предыдущими моментами времени и находящиеся в уравнении с текущими переменными.
    5) Предопределенными переменными динамической модели называют текущие и лаговые экзогенные переменные и лаговые эндогенные переменные, стоящие в уравнениях с текущими эндогенными переменными.
    
    Возможные ошибки спецификации модели:
    1. Неправильный выбор вида уравнения регрессии
    2. В уравнение регрессии включена лишняя (незначимая) переменная
    3. В уравнении регрессии пропущена значимая переменная
    77777
    RESET-тест Рамсея - это обобщенный тест на наличие следующих ошибок спецификации модели линейной регрессии:
    1) наличие пропущенных переменных. Регрессия содержит не все объясняющие переменные;
    2) неверная функциональная форма. Некоторые или все переменные должны быть преобразованы с помощью логарифмической, степенной, обратной или какой-либо другой функции;
    3) корреляция между фактором Х и случайной составляющей модели, которая может быть вызвана ошибками измерения факторов, рассмотрением систем уравнений или другими причинами.
    
    Тест Рамсея позволяет проверить, стоит ли начинать поиск дополнительной переменной для включения в уравнение
    1. Оценивается уравнение регрессии
    2. Вычисляются степени оценок зависимой переменной
    3. Оценивается уравнение регрессии с этими степенями
    4. Проводится оценка улучшения по F-критерию
        ''',

    '23. Уравнение регрессии в стандартизированном виде. Интерпретации стандартизированного коэффициента регрессии':
        r'''Параметры множественной регрессии можно определить другим способом, когда на основе матрицы парных коэффициентов корреляции строится уравнение регрессии в стандартизованном виде:\
    $t_y = \beta_1t_{x_1}+\beta_2t_{x_2} + \varepsilon$\
    где t - сдандартизованные переменные, для которых среднее значение равно 0, а среднее квадратическое отклонение равно 1;\
    $\beta$ - сдандартизованные коэфы регрессии
    77777
    Применяя МНК к уравнению множественной регрессии в стандартизованном масштабе, после соответствующих преобразований получим систему нормальных уравнений вида:\
    !система!\
    $r_{yx_1} = \beta_1 + \beta_2 r_{x_2x_1}$\
    $r_{yx_2} = \beta_1r_{x_2x_1} + \beta_2$\
    где $r_{yx_1}, r_{yx_2}$ - парные коэфы корреляции
    
    Стандартизированный коэффициент регрессии рассчитывается по формуле:\
    $\beta_j = b_j \dfrac{\sigma_{x_j}}{\sigma_y}$\
    $\beta_j$ - коэф при факторе $x_j$
    
    Определяет силу влияние вариации $x_j$ на вариацию результативного признака у при отвлечении от сопутствующего влияния вариаций других факторов, входящих в уравнение регрессии.
    ''',

    '24. Частные уравнения регрессии. Коэффициент эластичности (пример для различных математических функций).':
        r'''На основе линейного уравнения множественной регрессии:\
    $y = a +b_1x_1 + ... + b_px_p + e$\
    могут быть найдены частные уравнения регрессии:\
    $y_{x_1,..,x_p} = f(x_1)$,\
    $y_{x_2, x_1, x_3,..,x_p} = f(x_2)$,\
    ...\
    $y_{x_p, x_1,..,x_{p-1}} = f(x_p)$\
    т.е. уравнения регрессии, которые связывают результативный признак с соответствующими факторами $x_j$ при закреплении других учитываемых во множественной регрессии факторов на среднем уровне.
    
    В отличие от парной регрессии частные уравнения регрессии характеризуют изолированное влияние фактора на результат, ибо другие факторы закреплены на неизменном уровне. Эффекты влияния других факторов присоединены в них к свободному члену уравнения множественной регрессии. Это позволяет на основе частных уравнений регрессии определять частные коэффициенты эластичности:\
    $Э_{y_{x_i}} = b_i \dfrac{x_i}{\hat y_{x_i, x_1,.., x_{i-1}, x_{i+1}..,x_p}}$\
    где $b_i$ - коэффициенты регрессии для фактора х, в уравнении множественной регрессии;\
    $y_{x_i, x_1,.., x_{i-1}, x_{i+1}..,x_p}$ - частное уравнение регрессии.
    77777
    |Вид Функции|Средний коэф эластичности|
    
    $y = a+b*lnx + \varepsilon$\
    $\dfrac{b}{a+b*ln\bar x}$ 
    ---
    $y = e ^ {a+bx} * \varepsilon$ \
    $\bar x b$
    ---
    $y = \dfrac{1}{a+bx+\varepsilon}$\
    $\dfrac{b*c*\bar x}{b+e^{c*\bar x}}$
    ---
    $y = \dfrac{a}{1+b*e^{-cx+\varepsilon}}$\
    $\dfrac{b\bar x}{a+b\bar x}$
        ''',

    '25. Методы оценки параметров нелинейных моделей: метод прямого поиска':
        r'''Использование метода прямого поиска при нелинейном оценивании имеет определенные как преимущества, так 
    и недостатки по сравнению с другими методами. Его преимущества обусловлены достаточно несложной логической схемой расчетов и их относительной простотой. Его недостатком является небольшая скорость расчетов, что требует для получения результатов относительно много времени, особенно, когда количество оцениваемых параметров модели велико. Однако этот недостаток становится не слишком существенным при использовании компьютеров с высоким быстродействием.
        ''',

    '26. Методы оценки параметров нелинейных моделей: методы, основанные на линейной аппроксимации модели':
        r'''В основе этой группы методов лежит идея представления нелинейного функционала эконометрической модели f(alpha, x) в произвольной точке $а^{(0)}$ в виде линейной аппроксимирующей функции, например, ряда Тейлора. Это даст возможность определить в некотором смысле оптимальные приросты оценок параметров $\Delta a^{(1)}_i$, минимизирующие функцию суммы квадратов ошибки $S^2$  в окрестности точки $а^{(0)}$. 
    
    При этом, поскольку ряд Тейлора не является точной аппроксимацией функционала модели f(alpha, x), то новые значения оценок ее параметров, определяемые по очевидной формуле $a^{(1)_i} = а^{(0)} + \Delta a^{(1)}_i, i=0, 1,..,n$ могут не совпасть с наилучшими оценками. В таком случае необходимо аппроксимировать модель в точке $a^{(1)}$ и определить новые приросты параметров $\Delta a^{(2)}_i$ и соответствующие им оценки $a^{(2)}_i$ и т.д. Таким образом, формируется итеративная процедура последовательного приближения к искомым «оптимальным» оценкам $а_i$, которым соответствует локальный оптимум суммы квадратов ошибки в некоторой области существования значений параметров модели.
    Нахождение оценок параметров модели, соответствующих глобальному минимуму, может быть осуществлено с использованием такого же подхода, как и в методе прямого поиска, т.е. путем перебора различающихся вариантов исходных оценок параметров $а^{(0)}_i$, выбираемых из различных участков допустимой области их существования.
    
    В практике оценивания параметров нелинейных эконометрических моделей наибольшее распространение получил метод Гаусса-Зайделя. 
        ''',

    '27. Методы оценки параметров нелинейных моделей: методы, предполагающие линеаризацию целевой функции':
        r'''Среди методов, предполагающих линеаризацию целевой функции: градиентные методы, метод Макуардта. 
    
    В основе методов оценки параметров эконометрической модели, предполагающих линеаризацию целевой функции, т.е. суммы квадратов ошибки модели $S^2(а, x)$ по переменным $а_0, а_1...., а_n$, лежат свойства ее градиента $\nabla S^2$, согласно которым направление этого вектора в произвольной многомерной точке пространства параметров $а^j_0, а^j_1...., а^j_n$ указывает направление наибольшего роста функции $S^2(а, x)$ в этой точке. Соответственно противоположный вектор указывает на направление наибольшего уменьшения (наискорейшего спуска).
    
    Здесь следует подчеркнуть, что направление наискорейшего спуска в некоторой точке пространства параметров не обязательно указывает на точку оптимума функции $S^2(а, x)$. Однако двигаясь в этом направлении, можно попасть в следующую точку, в которой направление движения уточняется. А результате последовательность точек должны привести к искомому решению.
    
    Макуардт предложил комбинированный (компромиссный) метод оценки параметров эконометрической модели, объединяющий идеи методов Гаусса-Зайделя и наискорейшего спуска.
    В окрестности минимума метод Макуардта, как и другие методы, уменьшает длину прироста параметров.
        ''',

    '28. Тест на нормальность распределения вектора случайных возмущений.':
        r'''Тесты:
    - тест согласия Хельвига 
    - тест Шапиро-Вилька
    - тест Харка-Бера
    
    
    Тест согласия Хельвига\
    Принимается 2 гипотезы:\
    Но: случайные остатки имеют нормальное распределение;\
    Н21: случайные остатки не имеют нормального распределения.\
    Алгоритм\ 77777
    Шаг 1. Проводится оценка остатков регрессии
    по формуле:\
    $W_i = \dfrac{\varepsilon_i}{\hat S_u}$\
    где $\hat S_u$ - стандартное отклонение остатков
    
    Шаг 2. Полученные остатки ($W_i$) упорядочиваются по возрастанию.
    
    Шаг 3. По таблице функции нормального распределения выбирается значение функции $Ф(W_i)=P(W<W_i)$.
    
    Шаг 4. Определяются, так называемые, цели $I_i(i = 1, 2, ..., n)$, в роли которых выступают числовые интервалы шириной 1/n, образованные делением отрезка [о, 1] на n равных частей.
    77777
    Шаг 5. Значения функции $Ф(W_i)$ приписываются соответствующим целям, после чего определяется количество пустых целей, в которые не попало ни одно значение $Ф(W_i)$.
    
    
    Шаг 6. Из таблицы теста согласия Хельвига для данного количества наблюдений и и принятого уровня значимости а выбираются критические значения k1 и k2.
    
    Шаг 7. Проверяется соотношение\
    k1 < k < k2\
    где k - количество пустых целей.
    
    Если данное соотношение выполняется, то нет основания для отклонения гипотезы $H_0$, т.е. случайные отклонения носят нормальный характер распределения. Если же k<k1 или k>k2, то гипотеза $H_0$ отклоняется и принимается противоположная - $H_1$, т.е. случайные отклонения не имеют нормального характера распределения.
        ''',

    '29. Способы включения случайных возмущений в спецификацию нелинейной модели.':
        r'''1) Мультипликативное включение:\
    исходная спецификация: $Y_t = \alpha*X^\beta_t*\varepsilon_i$\
    Линеаризованная спецификация: $ln(Y_t) = ln(\alpha) + \beta*ln(X_t) + ln(\varepsilon)$
    
    2) Аддитивное включение:\
    исходная спецификация: $Y_t = \alpha*X^\beta_t*\varepsilon_i$\
    Логарифмическое преобразование: $ln(Y_t) = ln(\alpha*X^\beta_t*\varepsilon_i)$
        ''',

    '30. Теорема Гаусса-Маркова':
        r'''1. Математическое ожидание случайного отклонения $М(\varepsilon)$ должно быть равно 0 для всех наблюдений: $М(\varepsilon)$=0
    
    2. Дисперсия случайных случайных отклонений $D(\varepsilon)$постоянна для всex любых наблюдений: $D(\varepsilon_i)=D(\varepsilon_j)$ – гомоскедастичность.
    
    3. Случайные отклонения $\varepsilon_i$ и $\varepsilon_i$ не равны 0 для любых $i\neq j$ - отсутствие автокорреляции.
    
    4. Случайное отклонение должно быть независимо от объясняющей переменной.
    5. Модель является линейной относительно параметров.
    6. Отсутствие мультиколлинеарности.
    7. Случайная ошибка должна иметь нормальное распределение.
    
    Выполнение всех предпосылок при использовании МНК позволяет получить несмещенные оценки (BLUE - лучшие линейные несмещенные оценки)
        ''',

    '31.Свойства оценок параметров регрессии, полученных МНК':
        r'''- несмещенность(в этом случае математическое ожидание оценки совпадает с оцениваемым теоретическим параметром)
    - состоятельность(это означает, что для больших выборок вероятность значимых отклонений величины оценки от значения оцениваемого теоретического параметра равна нулю)
    - эффективность(чем меньше дисперсия оценки, тем она считается эффективнее)
        ''',

    '32. Гетероскедастичность: понятие, причины и последствия.':
        r'''Гетероскедастичность - это непостоянство дисперсий случайных возмущений для любых наблюдений
    
    Причины:
    Проблема гетероскедастичности характерна для перекрестных данных. В экономике она связана с эффектом масштаба.
    
    Последствия:
    1. Оценки коэффициентов регрессии не будут эффективными.
    2. Дисперсии оценок будут рассчитываться со смещением.
    3. Оценки, полученные на основе t- и F-статистик будут ненадежными.
        ''',

    '33. Метод выявления гетероскедастичности: графический, тест Бреуша - Пагана':
        r'''Наиболее простым в использовании является графический метод. Суть данного метода заключается в следующем: по оси абсцисс откладываются значения объясняющей переменной Х, а по оси ординат - либо остатки е,, либо их квадраты $e^2_i$
    
    - На рисунке (две горизонтальные прямые и точки между ними) все отклонения находятся внутри полуполосы постоянной ширины, параллельной оси абсцисс. Это говорит о независимости дисперсий от значений переменной Х и их постоянстве, то есть в этом случае мы имеем дело с гомоскедастичностью.
    
    - На рисунке (от 0 линия на 30 и 60 градусов, точки между ними) наблюдаются некие систематические изменения в соотношениях между значениями $х_i$, переменной Х и квадратами отклонений $e^2_i$. Рисунок соответствует рассмотренному выше примеру зависимости потребления от величины дохода. 
    - На рисунке (две линии под 45 градусов; точки между ними) отражена линейная. 
    - (парабола холмом не касаясь оси Х; точки вокруг линии) - квадратичная. 
    - (1/х в 1 четверти; точки вокруг линии) гиперболическая зависимости между квадратами отклонений $e^2_i$ и значениями $х_i$, объясняющей переменной Х. 
    
    Другими словами, ситуации, представленные на рисунках, отражают большую вероятность наличия гетероскедастичности для рассматриваемых статистических данных.
    
    77777
    #### Тест Бреуша-Пагана(Breusch-Pagan) 
    Предпосылкой теста является зависимость дисперсии возмущений от некоторых дополнительных переменных (или подмножества регрессоров из списка включенных в модель)
    
    $\sigma^2_t = \gamma_0+Z^T_t*\gamma$; t = 1,..,n;\
    где $Z_t = (z_{t_1},..,z_{t_p})^T$ - вектор независимых переменных
    
    $\gamma_0,\gamma=(\gamma_1,..,\gamma_p)^T$ - параметры.\
    Может быть выбрана и зависимость произвольной формы:\
    $\sigma^2_t = f(\gamma_0+Z^T_t*\gamma)$
    77777
    Алгоритм:\
    Шаг 1. По оцененной регрессионной модели
    вычисляется
    - вектор остатков
    - квадратов остатков
    - вектор квадратов остатков, нормированный на оценку дисперсии возмущений.
    
    Шаг 2. Оценка дисперсии возмущений вычисляется по формуле (метода максимального правдоподобия (ММП)-оценка)
    
    $\hat \sigma^2 = \dfrac{\sum_{t=1}^ne^2_t}{n}$
    77777
    Шаг 3. Проводится вспомогательная регрессия нормированных квадратов остатков на регрессоры
    
    $Z_t = (z_{t_1},..,z_{t_p})^T$
    
    $\dfrac{ e^2_t }{ \hat \sigma^2_t } = \gamma_0 + Z^T_t * \gamma+\nu_t$
    
    и вычисляется объяснённая часть вариации RSS. 
    
    Шаг 4. Статистика теста определяется по формуле:
    
    $BP = \dfrac{RSS}{2} \approx \chi^2(k)$
    
    где k - число регрессоров.
    Если $BP > \chi^2_\alpha$ , нулевая гипотеза о гомоскедастичности случайного возмущения не принимается.
        ''',

    '34. Метод выявления гетероскедастичности: Голдфелда-Куандта':
        r'''Тест Голфелда-Куандта\
    При малом объеме выборки, что наиболее характерно для эконометрических исследований, для оценки гетероскедастичности может использоваться метод Голфелда-Куандта, разработанный в 1965г. Голфелд и Куандт рассмотрели однофакторную линейную модель, для которой дисперсия остатков возрастает пропорционально квадрату фактора.
    
    Этапы теста\
    1) Все наблюдения n упорядочиваются по величине x
    2) Вся упорядоченная выборка разбивается на 3 части: k, (n-k), k\
    n = 20 $\rightarrow$ k = 8
    n = 30 $\rightarrow$ k = 11
    77777
    3) Оцениваются регрессии для первых k наблюдений и для третьих k наблюдений
    4) По каждой подвыборке рассчитвается сумма квадратов отклонений:\
    $S_1 = \sum_{i=1}^ke^2_i$\
    $S_3 = \sum_{i=n-k+1}^ke^2_i$
    5) рассчитывается F-расч статистика Фишера $F_{расч} = \dfrac{S_3}{S_1}$
    6) Сравнение F-расч и $F-кр(\alpha, \nu_1=\nu_2=k-m-1) \rightarrow$ гетероскедастичность
        ''',

    '35. Метод выявления гетероскедастичности: тест ранговой корреляции Спирмена':
        r'''Тест ранговой корреляции Спирмена
    
    Определяет существует ли связь между случайными отклонениями $\varepsilon^2_i$ и значениями фактора $x_i$ можно с помощью коэффициента ранговой корреляции Спирмена \
    $\rho = 1 - \dfrac{ 6\sum d^2_i }{ n(n^2 - 1) }$\
    $d_i$ - абсолютная разность между рангами значений $x_i$ и значений $\varepsilon_i$ по модулю 
    
    Основные этапы:
    1) нахождение случайных отклонений $\varepsilon_i$
    2) проранжировать $\varepsilon_i и x_i$. Получаем $R_x$ и $|R_e|$
    3) Нахождение d^2_i
    4) Нахождение коэффициента ранговой корреляции Спирмена $\rho$
    5) Проверка щначимости кожффициента корреляции $\rho$ с помощью t-статистики Стьюдента
    77777
    Значимость:
    1) Выдвигается две гипотезы:\
    $H_0: \rho=0$\
    $h_1: \rho\neq0$
    2) Рассчитывается t-расч статистика Стьюдента: $t_{расч} = \dfrac{ \rho^2 }{ \sqrt{1-\rho^2}}\sqrt{n-2}$
    3) Находим t-крит статистику Стьюдента из таблицы $(t_{кр}(\dfrac{\alpha}{2}, \nu=n-2)$ 
    4) Если t-расч > t-крит, то гипотеза $H_0$ отвергается, коэффициент Спирмена статистически значим, гетероскедастичность есть
        ''',

    '36. Метод выявления гетероскедастичности: тест Парка, Глейзера, Уайта.':
        r'''Тест Парка. Согласно тесту предполагается, что дисперсия $\sigma^2_{e_i}$ является функцией от i-го наблюдения объясняющей переменной $Х_j$.
    
    Применяется тест следующим образом:
    1. Строится уравнение регрессии $у_i = b_0 +b_1x_i+e_i$
    2. Для каждого наблюдения определяются $ln(e^2_i)$
    3. Строится регрессия $ln(e^2_i) = b_0 + b_1ln(x_i)+\nu_i$. В случае множественной регрессии подобная зависимость строится для каждой объясняющей переменной $Х_j$.
    4. Проверяется статистическая значимость коэффициента $b_1$, уравнения $ln(e^2_i) = b_0 + b_1ln(x_i)+\nu_i$, на основе t-критерия. Если коэффициент $b_1$, статистически значим, это означает наличие связи между $ln(e^2_i)$ и $ln(x_i)$, то есть для построенной модели $у_i = b_0 +b_1x_i+e_i$, характерна гетероскедастичности остатков.
    
    Тест Глейзера по своей сути аналогичен тесту Парка и дополняет его анализом других зависимостей между дисперсиями отклонений  $\sigma^2(e_i)$ и значениями переменной Х. 
    77777
    Применяется тест следующим образом: оценивается регрессионная зависимость абсолютных значений остатков $|е_i|$ от значений $х_i$, при этом рассматриваемая зависимость описывается следующим уравнением регрессии:\
    $|е_i| = b_0 +b_1x^k_i+\nu_i$
    
    Изменяя значения k, можно построить различные уравнения регрессии. Обычно k = -1; -0,5; 0,5; 1; 2; 3. Статистическая значимость коэффициента $b_1$, в каждом конкретном случае означает наличие гетероскедастичности. Если для нескольких регрессий коэффициент $b_1$ оказывается статистически значимым, то при определении характера зависимости обычно ориентируются на лучшую из них.
    
    Тест Уайта\
    Предполагается, что дисперсии $\sigma^2_{\varepsilon_i}$ с объясняющими переменными $X_j, j=\bar{1,m}$ в виде:\
    $\sigma^2_{\varepsilon_i} = f(X_{i_1}, X_{i_2},..,X_{i_m}+\eta_i, i=\bar{1,n}$\
    где f() - квадратичная функция от аргументов
    77777
    Тк дисперсии неизвестны, $\sigma^2_{\varepsilon_i}$, то их заменяют оценками квадратов отклонений $e^2_i$
    
    Этапы:
    1) Строится уравнение регрессии
    2) Оценивают вспомогательное уравнение регрессии
    3) Определяют из вспомогательного уравнения тестовую статистику ($U=nR^2$)
    4) Проверяют значимость уравнения с помощью критерия $\chi^2$. Если $U > \chi^2_{\alpha;k}$, то гипотеза гомоскедастичности отвергается. Число степеней свободы k равно числу объясняющих переменных вспомогательного уравнения.
        ''',

    '37. Метод смягчения гетероскедастичности':
        r'''Способы корректировки гетероскедастичности:
    1) взвешенный метод наименьших квадратов (ВМНК)
    2) доступный взвешенный метод наименьших квадратов (ДВМНК)
    3) обобщенный метод наименьших квадратов (ОМНК)
    
    Перечисленный методы нацелены на преобразование переменных таким образом, чтобы в спецификации преобразованной модели случайное возмущение удовлетворяло предпосылкам Гаусса-Маркова.
        ''',

    '38. ВМНК: дисперсии отклонений известны':
        r'''Переменные в преобразованной спецификации $Y^*_t = \sum_{j=1}^k\beta_jX^*_{jt}+\varepsilon^*_t$\
    определяется по формулам:\
    $Y^*_t = \dfrac{Y_t}{\sigma_t}$\
    $X^*_{jt} = \dfrac{X_{tj}}{\sigma_t}$\
    $\varepsilon^*_t = \dfrac{\varepsilon_t}{\sigma_t}$\
    где $\sigma_t$ - среднее квадратическое отклонение возмущения в наблюдении t\
    77777
    Определим количественные характеристики случайного возмущения $\varepsilon^*_t$ математическое ожидание:
    
    $E(\varepsilon^*_t) = \dfrac{E(\varepsilon_t)}{\sigma^2_t} = 0$
    
    Дисперсия случайного члена: $Var(\varepsilon^*_t) = Var(\dfrac{\varepsilon_t}{\sigma_t}) = \dfrac{Var(\varepsilon_t)}{\sigma^2_t} = \dfrac{\sigma^2_t}{\sigma^2_t} = 1$
    
    таким образом, случайное возмущение регрессионной модели с преобразованными переменными - гомоскедастично
        ''',

    '39. ВМНК: дисперсии отклонений неизвестны':
        r'''все то же самое как и в 38. ВМНК: дисперсии отклонений известны
    
    Только:
    1) Если $\sigma^2$ пропорциональна $x_i$: \
    $\sigma_i = \sqrt{x_i}$
    2) Если $\sigma^2$ пропорциональна $x^2_i$: \
    $\sigma_i = x_i$
        ''',

    '40. Мультиколлинеарность: понятие, причины и последствия':
        r'''Мультиколлинеарность в эконометрике - это явление, при котором две или более независимых переменных в множественной регрессионной модели высоко коррелированы между собой. Это означает, что одна переменная может быть линейно предсказана с помощью других с некоторой степенью точности.
    
    Причины мультиколлинеарности:
    1. Избыточные данные: Наличие в модели переменных, которые фактически являются комбинациями других переменных.
    2. Неправильная спецификация модели: Включение производных или вычисленных переменных, которые не являются независимыми.
    3. Ограниченные данные: Набор данных, в котором наблюдения слишком схожи или не охватывают достаточный диапазон значений переменных.
    77777
    Последствия мультиколлинеарности:
    1. Нестабильность оценок: Коэффициенты модели могут сильно изменяться при небольших изменениях в данных или спецификации модели.
    2. Высокие стандартные ошибки: Мультиколлинеарность увеличивает стандартные ошибки коэффициентов, что затрудняет определение их статистической значимости.
    3. Затрудненная интерпретация: Трудно интерпретировать влияние отдельных переменных на зависимую переменную, поскольку изменения одной переменной связаны с изменениями других.
    4. Проблемы с прогнозированием: Хотя общее качество модели может быть хорошим, точность прогнозов для новых данных может быть низкой.
    
    Для устранения или уменьшения мультиколлинеарности можно использовать различные методы, включая исключение некоторых переменных, применение методов регуляризации (например, гребневой регрессии или лассо) или использование анализа главных компонент для уменьшения размерности данных.
        ''',

    '41. Выявление мультиколлинеарности: коэффициент увеличения дисперсии (VIF–тест)':
        r'''Коэффициент увеличения дисперсии (VIF, от англ. Variance Inflation Factor) - это мера, используемая для выявления мультиколлинеарности в множественной регрессионной модели. Он показывает, насколько увеличивается дисперсия коэффициента оценки из-за линейной зависимости между объясняющими переменными.
    
    Как рассчитывается VIF::
    1. Для каждой независимой переменной $X_i$ в модели строится регрессионная модель, где $X_i$ выступает как зависимая переменная, а все остальные независимые переменные - как объясняющие.
    2. Для каждой такой регрессии рассчитывается коэффициент детерминации $R^2$.
    3. VIF для каждой переменной $X_i$ определяется как $VIF_i = \frac{1}{1 - R_i^2}$.
    77777
    Интерпретация VIF:
    - VIF, равный 1, означает отсутствие корреляции.
    - VIF между 1 и 5 обычно считается приемлемым.
    - VIF выше 5 может указывать на наличие мультиколлинеарности и требует дополнительного анализа.
    - VIF выше 10 часто считается указанием на серьезную мультиколлинеарность, требующую корректировки модели.
    77777
    Последствия высокого VIF:
    - Высокие значения VIF указывают на то, что коэффициенты модели становятся нестабильными и могут сильно меняться при небольших изменениях в данных.
    - Стандартные ошибки оценок коэффициентов увеличиваются, что затрудняет проверку гипотез о значимости коэффициентов.
    
    Для уменьшения мультиколлинеарности можно удалить переменные с высоким VIF или использовать методы регуляризации, такие как гребневая регрессия.
        ''',

    '42. Выявление мультиколлинеарности: Алгоритм Фаррара-Глобера':
        r'''Алгоритм Фаррара-Глобера — это статистический метод, используемый для выявления мультиколлинеарности в множественных регрессионных моделях. Этот алгоритм включает в себя тестирование на наличие мультиколлинеарности путем анализа корреляционной матрицы независимых переменных. Основные шаги алгоритма следующие:
    
    1. Корреляционная матрица: Рассчитывается матрица корреляции для всех независимых переменных. Это позволяет определить, насколько сильно переменные коррелируют друг с другом.
    
    2. Тест Фаррара-Глобера:
       - Определение значимости корреляций: Производится проверка значимости каждого коэффициента корреляции. Это делается с использованием t-теста на основе преобразованного значения корреляции Фишера.
       - Тест на наличие мультиколлинеарности: Дополнительно проводится тест на детерминант корреляционной матрицы. Если детерминант очень мал, это указывает на наличие мультиколлинеарности.
    77777
    3. Интерпретация результатов:
       - Значимые корреляции: Если многие пары переменных имеют значимо высокие коэффициенты корреляции, это указывает на возможную мультиколлинеарность.
       - Малый детерминант: Маленький детерминант корреляционной матрицы подтверждает наличие мультиколлинеарности между переменными.
    
    Алгоритм Фаррара-Глобера полезен для выявления общей мультиколлинеарности в модели, но может не всегда быть эффективным для определения конкретных переменных, вызывающих эту проблему. Для устранения мультиколлинеарности можно попытаться исключить некоторые из коррелирующих переменных или использовать методы регуляризации.
        ''',

    '43. Построение гребневой регрессии. Суть регуляризации.':
        r'''Гребневая регрессия (ridge regression) — это метод регуляризации, используемый для анализа множественных регрессионных данных, когда независимые переменные коррелируют между собой (мультиколлинеарность). Этот метод помогает уменьшить переобучение и повысить стабильность оценок коэффициентов.
    
    Построение гребневой регрессии:
    1. Модификация функции потерь: В классической регрессии минимизируется сумма квадратов остатков. В гребневой регрессии к этой сумме добавляется штрафное слагаемое, равное квадрату нормы вектора коэффициентов, умноженному на параметр регуляризации (λ).
    
    Математически, функция потерь имеет вид: Сумма квадратов остатков + $\lambda * \sum_{i=1}^n \beta_i^2$
    
    2. Выбор параметра регуляризации (λ): Параметр λ контролирует степень регуляризации. При λ = 0, гребневая регрессия сводится к обычной регрессии. При увеличении λ уменьшается величина коэффициентов, что помогает уменьшить переобучение и повышает устойчивость модели.
    77777
    3. Оценка коэффициентов: С помощью методов оптимизации (например, градиентного спуска) находятся коэффициенты, минимизирующие модифицированную функцию потерь.
    
    Суть регуляризации:
    - Регуляризация — это метод, который добавляет дополнительные ограничения или штрафы к модели для предотвращения переобучения и улучшения обобщающей способности модели на новых данных.
    - Гребневая регрессия, как форма регуляризации, штрафует большие коэффициенты, уменьшая их величину. Это помогает снизить влияние мультиколлинеарности и делает модель более стабильной.
    
    Гребневая регрессия особенно полезна, когда имеется много переменных, и некоторые из них коррелированы. Она помогает сохранить все переменные в модели, одновременно уменьшая проблемы, связанные с мультиколлинеарностью.
        ''',

    '44. Алгоритм пошаговой регрессии':
        r'''Алгоритм пошаговой регрессии включает в себя выбор наилучших переменных для регрессионной модели и состоит из следующих шагов:
    
    1. Начальный этап: Определите критерий для включения переменных (например, уровень значимости 0.05). Начните либо с пустой модели (без переменных), либо с полной (со всеми переменными).
    
    2. Добавление переменных (Forward selection): Последовательно добавляйте переменные, выбирая те, которые наиболее улучшают модель (например, увеличивают R² или имеют наименьшее p-value). Продолжайте, пока улучшения модели не станут незначительными.
    77777
    3. Удаление переменных (Backward elimination): Если начинаете с полной модели, исключайте переменные, которые наименее влияют на модель или не оправданы по критерию значимости. Продолжайте, пока каждая переменная в модели не окажется оправданной.
    
    4. Шаговая регрессия (Stepwise regression): Комбинируйте добавление и удаление переменных. После каждого добавления проверяйте, не стали ли другие переменные незначительными, и при необходимости исключайте их. Продолжайте, пока не достигнете оптимальной модели.
    
    Этот метод помогает сформировать более простую и эффективную модель, но может привести к проблемам с обобщаемостью и игнорированию важных взаимодействий между переменными
        ''',

    '45. Метод главных компонент как радикальный метод борьбы с мультиколлинеарностью':
        r'''Метод главных компонент (Principal Component Analysis, PCA) может использоваться как радикальный метод борьбы с мультиколлинеарностью в данных. Мультиколлинеарность возникает, когда независимые переменные в модели регрессии сильно коррелируют между собой, что может затруднить оценку параметров и интерпретацию результатов. 
    МГК предоставляет способ уменьшить размерность данных и снизить влияние мультиколлинеарности. 
    
    Основные шаги МГК:
    1. Стандартизация переменных: \
    Переменные стандартизируются (центрируются и шкалируются) так, чтобы они имели среднее значение 0 и стандартное отклонение 1. Это важно, т.к. МГК чувствителен к масштабу переменных.\
    2. Расчет матрицы ковариации: \
    Вычисляется матрица ковариации для стандартизированных переменных.\
    3. Расчет собственных значений и собственных векторов: \
    77777
    Собственные значения и соответствующие им собственные векторы вычисляются из матрицы ковариации. Собственные векторы указывают направления максимальной изменчивости в данных, а собственные значения измеряют величину этой изменчивости.\
    4. Выбор главных компонент: \
    Главные компоненты выбираются в порядке убывания собственных значений. Обычно, выбирают только те компоненты, которые объясняют большую часть изменчивости (например, 95% или 99%).\
    5. Преобразование данных: 
    Исходные данные проецируются на пространство главных компонент. Новые переменные (главные компоненты) создаются как линейные комбинации исходных переменных.
    
    Использование метода главных компонент может помочь снизить мультиколлинеарность, так как новые переменные (главные компоненты) являются ортогональными друг другу, что уменьшает корреляцию между ними. Но интерпретация результатов становится более сложной, так как главные компоненты часто не имеют прямого экономического или практического смысла.
        ''',

    '46. Устранение мультиколлинеарности':
        r'''1) Отбор наиболее информативных признаков для включения в модель;
    2) Построение регрессионной модели в условиях мультиколлинеарности без изменения состава регрессоров\
    $\beta = (X.T * X)^{-1} * X.T * y$
    
    Когда матрица X.T*X близка к вырожденной (det = 0), то для вычисления коэффициентов регрессии при тесной взаимосвязи факторных признаков можно искусственно улучшить обусловленность данной матрицы путем прибавления к ней некоторого коэф. tau диагональной матрицы L, элементы которой являются X.T*X диаг.
    X.T*X + tau*L (процедура регуляризации)
    77777
    Обращение матрицы (X.T*X + tau*L) при не слишком малой величине tau не вызывает особых затруднений, при этом на элементы матрицы обратной данной слабой влияют.
    Небольшие изменения исходных данных, обусловленные как погрешностью измерений, так и ошибками, возникающими из-за вычислений, связанных с разрядностью сетки.
    
    Ridge-Regression (гребневая регрессия) устраняет и понижает размерность.
    B = (X.T * X)^(-1) * X.T * y
    
    3) Переход к ортогональным признакам
    Наиболее эффективным методом преодоления проблемы мультиколлинеарности можно считать переход от исходных признаков к новым взаимноортогональным методом главных компонентов.
    ''',

    '47. Автокорреляция: понятие, виды, причины и последствия':
        r'''Автокорреляция – это наличие сильной корр. зависимости между последовательными уровнями одного ряда динамики.
    Автокорреляция это нарушение третьего условия Гаусса-Маркова.
    Существует 2 вида автокорреляции:
    1) Положительная автокорреляция, т.е. $\sigma(\varepsilon_{t-1};\varepsilon_t) > 0$
    2) Отрицательная автокорреляция, т.е. $\sigma(\varepsilon_{t-1};\varepsilon_t) < 0$
    77777
    Причины автокорреляции:
    - инертность экономических показателей;
    - эффект паутины;
    - характер наблюдений (например, данные временных рядов);
    - ошибки спецификации модели (пропуск важной объясняющей переменной, использование ошибочной функциональной зависимости между переменными);
    - манипулирование данными, сглаживание данных.
    
    Последствия:
    1) хотя оценки МНК коэф. регрессии останутся несмещенными, они не будут эффективными;
    2) оценки МНК для стандартных отклонений коэффициента будут смещенными, чаще всего вниз, то есть будут заниженными
    3) t- и F-статистики будут неадекватными. Следствием заниженности оценок стандартных отклонений коэф. является завышенность t-статистик.
    ''',

    '48. Методы определения автокорреляции: графический':
        r'''1) Строится последовательно-временной график
    - по оси абсцисс откладывается время получения статистических данных либо порядковый номер наблюдения
    - а по оси ординат - отклонения $e_t$
    2) Анализируется наличие связи между остатками наблюдений:
    - на первых трех рисунках видны определенные связи между отклонениями, т.е. имеет место автокорреляция
    - на последнем рисунке ее скорее всего нет
    77777
    3) рисунки (точки около линий):
    - линия под 45 градусов, касается оси х посередине рисунка
    - сделать параболу ямкой, минимум немного ниже оси х
    - функция как y=sin(x)
    - точки примерно распределены по оси х
        ''',

    '49. Методы определения автокорреляции: метод рядов':
        r'''Ряд – это непрерывная последовательность одинаковых знаков. Длина ряда – количества знаков в ряду. n1- общее количество знаков “+” при n наблюдениях (количество положительных отклонений $e_t$); n2 – количество знаков “-”; 
    k – количество рядов. 
    
    При достаточно большом количестве наблюдений (n1 > 10, n2 > 10) и отсутствии автокорреляции k имеет асимптотически нормальное распределение:\
    $M(k) = \frac{2n_1n_2}{n_1+n_2}$\
    $D(k) = \frac{2n_1n_2(2n_1n_2-n_1-n_2)}{(n_1+n_2)^2(n_1+n_2-1)}$
    77777
    Тогда, если $M(k) - u_{a/2}D(k) < k < M(k) + u_{a/2}D(k)$, то гипотеза об отсутствии автокорреляции не отклоняется 
    При небольшом числе наблюдений (n1 < 20, n2 < 20) Свед и Эйзенхарт разработали таблицы критических значений количества рядов при n наблюдениях.
    
    Суть таблиц в следующем. На пересечении строки n1 и столбца n2 определяются нижнее k1 и верхнее k2 значения при уровне значимости a = 0.05\
    Если k1 < k < k2 , то говорят об отсутствии автокорреляции.\
    Если k ≤ k1, (k ближе к 0) то говорят о положительной автокорреляции остатков.\
    Если k ≥ k2   (k ближе к 4) то говорят об отрицательной автокорреляция остатков.
        ''',

    '50. Методы определения автокорреляции: критерий Дарбина Уотсона':
        r'''Суть его состоит в вычислении статистики DW Дарбина–Уотсона и на основе ее величины − осуществлении выводов об автокорреляции.
    
    $DW = \frac{\sum_{t=2}^T(e_t - e_{t-1})^2}{\sum_{t=1}^Te^2_t}$
    
    статистика Дарбина-Уотсона тесно связана с выборочным коэффициентом корреляции Гера
    
    $DW \approx 2(1-r_{e_te_{t-1}})$ 
    77777
    Таким образом, 0 ≤ DW ≤ 4 и его значения могут указать на наличие либо отсутствие автокорреляции. Действительно, если $r_{e_te_{t-1}} \approx 0$ (автокорреляция отсутствует), то DW = 2. 
    
    Если $r_{e_te_{t-1}} \approx 1$ (положительная автокорреляция), то DW ≈ 0. \
    Если $r_{e_te_{t-1}} \approx -1$ (отрицательная автокорреляция), то DW ≈ 4.
    
    
    Общая схема критерия Дарбина–Уотсона будет следующей:
    1. По построенному эмпирическому уравнению регрессии 
    $y_t = b_0 + b_1x_{t_1} + ... + b_m x_{t_m}$  \
    определяются значения отклонений $е_t = у_t − \hat y_t$  для каждого наблюдения t.
    2. По формуле $DW = \frac{\sum_{t=2}^T(e_t - e_{t-1})^2}{\sum_{t=1}^Te^2_t}$ рассчитывается статистика DW.
    77777
    3. По таблице критических точек Дарбина–Уотсона определяются два числа $d_l$ и $d_u$ и осуществляют выводы по следующей схеме: 0 ≤ DW < dl − существует положительная автокорреляция,
    $d_l$ ≤ DW < $d_u$ − вывод о наличии автокорреляции не определен, $d_u$ ≤ DW < 4 − $d_u$ − автокорреляция отсутствует,
    4 − $d_u$ ≤ DW < 4 − $d_l$ − вывод о наличии автокорреляции не определен, 4 − $d_l$ ≤ DW ≤ 4 − существует отрицательная автокорреляция.
    
    При использовании критерия Дарбина–Уотсона необходимо учитывать следующие ограничения.
    1. Критерий DW применяется лишь для тех моделей, которые содержат свободный член.
    2. Предполагается, что случайные отклонения $\varepsilon_t$ определяются по следующей итерационной схеме $\varepsilon_t = \rho\varepsilon_{t-1} + \nu_t$, называемой авторегрессионной схемой первого порядка AR(1). Здесь $\nu_t$ − случайный член.
    3. Статистические данные должны иметь одинаковую периодичность (т. е. не должно быть пропусков в наблюдениях).
    4. Критерий Дарбина–Уотсона не применим для регрессионных моделей, содержащих в составе объясняющих переменных зависимую переменную с временным лагом в один период, т. е. для так называемых авторегрессионных моделей вида:
    
    $y_t = \beta_0 + \beta_1x_{t_1} + ... + \beta_mx_{t_m} + \gamma Y_{t−1} + \varepsilon_t$
        ''',

    '51. Авторегрессионная схема первого порядка':
        r'''$X_t = c+ rX_{t-1} + \varepsilon_t$
    
    Авторегрессионная модель первого порядка, обозначаемая как AR(1) или авторегрессия первого порядка.
    
    Авторегрессионная схема первого порядка (AR(1)) - это специальный случай авторегрессионной модели, который используется в эконометрике для моделирования временных рядов. Этот подход предполагает, что текущее значение зависимой переменной зависит от двух предыдущих значений.
        ''',

    '52. Авторегрессионная схема второго порядка':
        r'''Авторегрессионная схема второго порядка (AR(2)) - это специальный случай авторегрессионной модели, который используется в эконометрике для моделирования временных рядов. Этот подход предполагает, что текущее значение зависимой переменной зависит от двух предыдущих значений.
    
    Общий вид AR(2) модели может быть представлен следующим образом:
    
    $X_t = c+ a_1X_{t-1} + a_2X_{t-2} + \varepsilon_t$
    77777
    Авторегрессионная схема второго порядка может быть использована для моделирования временных рядов с периодической или колеблющейся тенденцией. Она также может быть использована для моделирования данных, которые имеют сезонные компоненты или другие периодические изменения.
    
    Подразумевается, что в модели регрессии ошибки специфицированы как $\varepsilon_t = \rho\varepsilon_{t-1} + u_t$
    
    Преимущества AR(2) модели:
    1. удобна для моделирования временных рядов с периодической или колеблющейся тенденцией.
    2. модель является простой и понятной, что делает ее удобной для использования и понимания
    77777
    Недостатки AR(2) модели:
    
    1. Модель может быть ограничена в своей способности обработать более сложные временные ряды, которые могут иметь более высокую степень нелинейности 
    
    2. Модель предполагает, что ошибки в модели независимы и имеют одинаковую дисперсию, что может не соответствовать реальным условиям.
    3. Неспособность обработать зависимости между независимыми переменными
    
    Несмотря на эти недостатки, AR(2) модель все еще широко используется в эконометрике из-за ее простоты и удобства в применении. Для устранения некоторых из этих недостатков были разработаны более сложные модели, такие как модели с нелинейностью, модели с неравномерностью, модели с зависимостями между независимыми переменными и другие.
        ''',

    '53. Метод определения коэффициента ρ: на основе статистики Дарбина-Уотсона':
        r'''Подразумевается, что в модели регрессии ошибки специфицированы как $\varepsilon_t = \rho\varepsilon_{t-1} + u_t$
    
    Можно показать, что для авторегрессии первого порядка статистика Дарбина-Уотсона связана с выборочным коэффициентом автокорреляции между соседними наблюдениями временного ряда $r_{e_te_{t-1}}$ следующим отношением
    
    $DW \approx 2(1-r_{e_te_{t-1}})$ 
    
    Обычно значение   рассчитывается по формуле $\hat\rho = 1 - \frac{DW}{2}$
        ''',

    '54. Метод определения коэффициента ρ: Кохрана-Оркатта, Хилдрета-Лу':
        r'''Метод Кохрана-Оркатта\
    
    1. Задается уровень точности $\delta\in(0;0,1)$ и начальное значение коэффициента корреляции $\rho = 0$, которое подставляется в систему уравнений наблюдений и с помощью МНК оцениваются параметры линейной авторегрессионной модели.
    2. Оцениваются значения компонент вектора случайных возмущений: $\bar u = \{ u_1,..,u_n\}$
    3. Из компонент вектора и формируется система уравнений наблюдений вида \
    !система!\
    $u_2 = \rho u_1 + \varepsilon_2$,\
    ...\
    $u_n = \rho u_{n-1} + \varepsilon_n$\
    77777
    по которой находится очередная МНК-оценка параметра р
    4. Очередное значение коэффициента корреляции сравнивается с предыдущим и, если выполняется условие, $|\rho_i - \rho_{i-1}| <= \delta$ то итерационный процесс прекращается, а в качестве решения принимаются последние значения оценок параметров и коэффициента корреляции. Если условие по точности не выполняется, то процедура продолжается.
    5. С очередным значением параметра $\rho$; строится очередная система уравнений наблюдений 
    6. Вновь вычисляются значения оценок параметров линейной модели и переходят к п. 2 алгоритма.
    
    Метод Хилдрета - Лу:\
    Имеется регрессия вида: $у^* = \beta^*_0 +\beta^*_1x_t^* + \nu_t$, Данная регрессия оценивается для каждого возможного значения $\rho$ из отрезка [+1;-1] с любым шагом. Величина $\hat\rho$ дающая наименьшую стандартную ошибку регрессии принимается в качестве оценки коэффициента $\rho$. Значение $\beta^*_0$ и $\beta_1$ оцениваются из уравнения регрессии вида: $у^* = \beta^*_0 +\beta^*_1x_t^* + \nu_t$, именно с данным значение $\hat\rho$
        ''',

    '55. Метод определения коэффициента ρ: на основе первых разностей':
        r'''Этот метод основан на расчете первых разностей между наблюдаемыми значениями и последующем определении коэффициента корреляции между этими разностями. Процесс определения коэффициента ρ на основе первых разностей включает следующие шаги: 
    1. Рассчитать первые разности между наблюдаемыми значениями последовательности. Первая разность для каждого наблюдения равна разности между соседними наблюдаемыми значениями.
    2. Рассчитать коэффициент корреляции между первыми разностями. Это может быть сделано с помощью любого стандартного метода оценки коэффициента корреляции, такого как метод Спирмена, метод Кенни или метод Скотта.
    3. Интерпретировать полученный коэффициент корреляции. Значение коэффициента корреляции, лежащего в диапазоне от -1 до 1, позволяет оценить силу взаимосвязи между последовательностями наблюдаемых значений. Значение, близкое к 1, указывает на сильную положительную корреляцию, значение, близкое к -1, указывает на сильную отрицательную корреляцию, а значение, близкое к 0, указывает на слабую корреляцию.
    
    Оценка коэффициента ρ на основе первых разностей имеет несколько преимуществ. Во-первых, этот метод не требует предварительного знания функции корреляции между наблюдаемыми значениями. Во-вторых, этот метод может быть применен к различным типам данных, таким как целые числа, вещественные числа и даже некоторые категориальные данные. Однако метод определения коэффициента ρ на основе первых разностей также имеет некоторые ограничения. Например, этот метод может быть менее точным, если взаимосвязи между наблюдаемыми значениями изменяются со временем или в зависимости от других факторов. Кроме того, этот метод может быть менее эффективным при анализе длинных последовательностей наблюдаемых значений, где взаимосвязи между значениями могут меняться со временем.
        ''',

    '56. Фиктивная переменная и правило её использования':
        r'''Фиктивные (искусственные) переменные – это переменные с дискретным множеством значений, которые количественным образом описывают качественные признаки. В эконометрических моделях обычно используются фиктивные переменные бинарного типа  «0-1»:
    Существует правило использования фиктивных переменных: если качественная переменная имеет k альтернативных значений, то при моделировании используются только (k-1) фиктивная переменная. 
        ''',

    '57. Модель дисперсионного анализа':
        r'''Регрессионные модели, содержащие лишь качественные объяс- няющие переменные, называются ANOVA-моделями (моделями дисперсионного анализа).\
    Например, пусть У - начальная заработная плата.\
    D = система
    - 0, если претендент не имеет высшего образования
    - 1, если претендент имеет высшее образование
    
    Тогда зависимость можно выразить моделью парной регрессии\
    $Y = \beta_0 + \gamma D + \varepsilon$
    77777
    Очевидно, \
    М(Ү | D = 0) = $\beta_0+\gamma*0=\beta_0$,\
    M(Y | D = 1) = $\beta_0+\gamma*1=\beta_0 + \gamma$
    
    При этом коэффициент $\beta_0$ определяет среднюю начальную заработную плату при отсутствии высшего образования. Коэффициент $\gamma$ указывает, на какую величину отличаются средние начальные заработные платы при наличии или отсутствии высшего образования у претендента. Проверяя статистическую значимость коэффициента $\gamma$ с помощью t-статистики либо значимость коэффициента детерминации $R^2$ с помощью F-статистики, можно определить, влияет или нет наличие высшего образования на начальную заработную плату.
    
    Нетрудно заметить, что ANOVA-модели представляют собой кусочно-постоянные функции. Однако такие модели в экономике крайне редки. Гораздо чаще встречаются модели, содержащие как качественные, так и количественные переменные.
        ''',

    '58. Модель ковариационного анализа':
        r'''Это ANCOVA модель
    
    ANCOVA оценивает, равны ли средние значения зависимой переменной по уровням одной или нескольких категориальных независимых переменных и по одной или нескольким непрерывным переменным. Например, категориальная переменная может описывать лечение, а непрерывная переменная может быть ковариатами или неприятными переменными; или наоборот
        ''',

    '59. Фиктивные переменные в сезонном анализе':
        r'''Использование фиктивных переменных в сезонном анализе\
    Многие экономические показатели напрямую связаны с сезон- ными колебаниями. Например, спрос на туристические путевки, ох- лажденную воду и мороженое существенно выше летом, чем зимой. Спрос на обогреватели, шубы выше зимой. Некоторые показатели имеют существенные квартальные колебания и т. д.\
    Обычно сезонные колебания характерны для временных рядов. Устранение или нейтрализация сезонного фактора в таких моделях позволяет сконцентрироваться на других важных количественных и качественных характеристиках модели, в частности на общем направ- лении развития модели, так называемом тренде. Такое устранение се- зонного фактора называется сезонной корректировкой. Существует несколько методов сезонной корректировки, одним из которых явля- ется метод фиктивных переменных.\
    Пусть переменная Y определяется количественной переменной Х, причем эта зависимость существенно разнится по кварталам. Тогда общую модель в этой ситуации можно представить в виде:\
    $Y_t = в_0 + в_1X_t + г_1D_{1t} + г_2D_{2t} + г_3D_{3t} + e_t $ (*)
    77777
    где $D_{1t}$ - система
    - 1, если рассматривается II квартал,
    - 0, в противном случае.
    
    где $D_{2t}$ - система
    - 1, если рассматривается III квартал,
    - 0, в противном случае.
    
    где $D_{3t}$ - система
    - 1, если рассматривается IV квартал,
    - 0, в противном случае.
    77777
    Заметим, что число кварталов равно четырем, а следовательно число фиктивных переменных должно быть равно трем. В нашем примере в качестве базы выбран I квартал. Если значения У существенно различаются по кварталам (сезонам), то в уравнении (*) коэффициенты при фиктивных переменных окажутся статистически значимыми. Тогда ожидаемое значение Y по кварталам определяется следующими соотношениями:\
    M(Y |D1 = 0, D2 = 0, D3 = 0) = $в_0 +в_1Х$           - для І квартала,\
    MY |D1 = 1, D2 = 0, D3 = 0) = $(в_0 + г_1) + в_1Х$    - для ІІ квартала,\
    M(Y|D1 = 0, D2 =1, D3 = 0)= $(в_0 +г_2) +в_1Х$        - для ІІІ квартала,\
    M(Y|D1 = 0, D2 = 0, D3 = 1) = $(в_0 + г_3) + в_1Х$     - для ІV квартала.
    77777
    Легко видеть, что в модели (*) рассматриваются такие ситуации, при которых квартальные различия отражаются лишь в различии свободных членов моделей. Если же различия затрагивают и изменения коэффициента пропорциональности, то это может быть отражено следующей моделью:\
    $Y_t = в_0 + в_1X_t + г_1D_{1t} + г_2D_{2t} + г_3D_{3t} + г_4D_{1t}X_t + г_5D_{2t}X_t + г_6D_{3t}X_t + e_t $ (2)
    
    Выбор правильной формы модели регрессии является в данной ситуации достаточно серьезной проблемой, т. к. в этом случае вполне вероятны ошибки спецификации. Наиболее рациональной практиче- ской стратегией выбора модели является следующая схема.
    Вначале рассматривается модель (2). Определяется статисти- ческая значимость коэффициентов. Если дифференциальные угловые коэффициенты оказываются статистически незначимыми, то переходят к модели (*). Если в этой модели дифференциальные свобод-
    ные члены оказываются статистически незначимыми, то делают вывод, что квартальные (сезонные) изменения несущественны для рассматриваемой зависимости.
        '''

}


def spisok():
    return tasks


def q(a):
    a = a.lower().split()
    for i in tasks.keys():
        flag = True
        for f in a:
            if f not in i.lower():
                flag = False
        if flag:
            res = tasks[i].split('77777')
            return (len(res), res)
    print('-')


def p(a, k=1):
    pc.copy(a[1][k-1])


def base(a=0):
    if a == 0:
        print('1) base1')
        print('2) info')
        print('3) гомоскедастичность')
        print('4) graph')
        print('5) prediction')
    elif a == 1:
        s = '''
import pandas as pd
import statsmodels.api as sm
import numpy as np

df = pd.DataFrame({'y' : [6580, 6916, 5908, 7168, 7420, 6272, 6804, 7084, 7140, 6860, 6412, 6048, 6720, 6972, 7252],
                  'x1' : [2801, 2910, 2676, 2956, 3078, 2734, 2856, 2924, 2935, 2884, 2761, 2704, 2840, 2916, 2964],
                  'x2' : [1178, 1388, 863 , 1455, 1838, 1005, 1275, 1395, 1410, 1343, 1110, 930 , 1245, 1388, 1500]})

# надо взять кроме первого элемента - выпиливаем
X = sm.add_constant(df.drop(['y'], axis=1))[1:]
Y = df['y'][1:]

# если коэфы в степенях - логарифмировать
X = np.log(X)
Y = np.log(Y)

model = sm.OLS(Y, X).fit()
model.summary()'''
        pc.copy(s)
        return ''

    elif a == 2:
        s = '''
R^2 - адекватность модели
t > tтабл - статистически значимы
p-value < 0.05 - статистически значимы
F > Fтабл - фишер
Durbin-Watson +-2 автокорреляция
    '''
        pc.copy(s)
        return ''

    elif a == 3:
        s = '''
import scipy.stats as sc

stat, p_value = sc.shapiro(model.resid)
print(f'Statistics={stat}, p-value={p_value}')

stat, p_value = sc.normaltest(model.resid)
print(f'Statistics={stat}, p-value={p_value}')

# Тест Голдфельда-Квандта на гетероскедастичность
from statsmodels.stats.diagnostic import het_goldfeldquandt
test_statistic, p_value, _ = het_goldfeldquandt(model.resid, model.model.exog)

print(f'Test Statistic: {test_statistic}')
print(f'p-value: {p_value}')
# p>0.05 - гомоскедастична, нормальное распределение остатков
        '''
        pc.copy(s)
        return ''

    elif a == 4:
        s = '''
import matplotlib.pyplot as plt

# Визуализация Q-Q plot
sm.qqplot(model.resid, line='s')
plt.title('Q-Q Plot of Residuals')
plt.show()
        '''
        pc.copy(s)
        return ''

    elif a == 5:
        s = '''
# вместо текста - данные; если логарифмировали - повторить
model.get_prediction(['const', 'x1', 'x2']).summary_frame(alpha=0.05)
model.get_prediction(np.log(sm.add_constant(df.drop(['y'], axis=1))).iloc[0]).conf_int()

        '''
        pc.copy(s)
        return ''

