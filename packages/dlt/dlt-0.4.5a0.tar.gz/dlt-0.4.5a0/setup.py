# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['dlt',
 'dlt.cli',
 'dlt.common',
 'dlt.common.configuration',
 'dlt.common.configuration.providers',
 'dlt.common.configuration.specs',
 'dlt.common.data_types',
 'dlt.common.data_writers',
 'dlt.common.destination',
 'dlt.common.json',
 'dlt.common.libs',
 'dlt.common.normalizers',
 'dlt.common.normalizers.json',
 'dlt.common.normalizers.naming',
 'dlt.common.reflection',
 'dlt.common.runners',
 'dlt.common.runtime',
 'dlt.common.schema',
 'dlt.common.storages',
 'dlt.common.storages.fsspecs',
 'dlt.destinations',
 'dlt.destinations.impl',
 'dlt.destinations.impl.athena',
 'dlt.destinations.impl.bigquery',
 'dlt.destinations.impl.databricks',
 'dlt.destinations.impl.duckdb',
 'dlt.destinations.impl.dummy',
 'dlt.destinations.impl.filesystem',
 'dlt.destinations.impl.motherduck',
 'dlt.destinations.impl.mssql',
 'dlt.destinations.impl.postgres',
 'dlt.destinations.impl.qdrant',
 'dlt.destinations.impl.redshift',
 'dlt.destinations.impl.snowflake',
 'dlt.destinations.impl.synapse',
 'dlt.destinations.impl.weaviate',
 'dlt.extract',
 'dlt.extract.incremental',
 'dlt.helpers',
 'dlt.helpers.dbt',
 'dlt.helpers.dbt_cloud',
 'dlt.load',
 'dlt.normalize',
 'dlt.pipeline',
 'dlt.reflection',
 'dlt.sources',
 'dlt.sources.helpers',
 'dlt.sources.helpers.requests']

package_data = \
{'': ['*']}

install_requires = \
['PyYAML>=5.4.1',
 'SQLAlchemy>=1.4.0',
 'astunparse>=1.6.3',
 'click>=7.1',
 'fsspec>=2022.4.0',
 'gitpython>=3.1.29',
 'giturlparse>=0.10.0',
 'hexbytes>=0.2.2',
 'humanize>=4.4.0',
 'jsonpath-ng>=1.5.3',
 'makefun>=1.15.0',
 'packaging>=21.1',
 'pathvalidate>=2.5.2',
 'pendulum>=2.1.2',
 'pytz>=2022.6',
 'requests>=2.26.0',
 'requirements-parser>=0.5.0',
 'semver>=2.13.0',
 'setuptools>=65.6.0',
 'simplejson>=3.17.5',
 'tenacity>=8.0.2',
 'tomlkit>=0.11.3',
 'typing-extensions>=4.0.0',
 'tzdata>=2022.1']

extras_require = \
{':os_name == "nt"': ['win-precise-time>=1.4.2'],
 ':platform_python_implementation != "PyPy"': ['orjson>=3.6.7,<=3.9.10'],
 'athena': ['pyarrow>=12.0.0',
            's3fs>=2022.4.0',
            'botocore>=1.28',
            'pyathena>=2.9.6'],
 'az': ['adlfs>=2022.4.0'],
 'bigquery': ['grpcio>=1.50.0',
              'google-cloud-bigquery>=2.26.0',
              'pyarrow>=12.0.0',
              'gcsfs>=2022.4.0'],
 'cli': ['cron-descriptor>=1.2.32', 'pipdeptree>=2.9.0,<2.10'],
 'databricks': ['databricks-sql-connector>=2.9.3,<3.0.0'],
 'dbt': ['dbt-core>=1.2.0',
         'dbt-redshift>=1.2.0',
         'dbt-bigquery>=1.2.0',
         'dbt-duckdb>=1.2.0',
         'dbt-snowflake>=1.2.0',
         'dbt-athena-community>=1.2.0',
         'dbt-databricks>=1.7.3,<2.0.0'],
 'duckdb': ['duckdb>=0.6.1,<0.10.0'],
 'filesystem': ['s3fs>=2022.4.0', 'botocore>=1.28'],
 'gcp': ['grpcio>=1.50.0', 'google-cloud-bigquery>=2.26.0', 'gcsfs>=2022.4.0'],
 'gs': ['gcsfs>=2022.4.0'],
 'motherduck': ['pyarrow>=12.0.0', 'duckdb>=0.6.1,<0.10.0'],
 'mssql': ['pyodbc>=4.0.39,<5.0.0'],
 'parquet': ['pyarrow>=12.0.0'],
 'postgres': ['psycopg2-binary>=2.9.1'],
 'postgres:platform_python_implementation == "PyPy"': ['psycopg2cffi>=2.9.0'],
 'qdrant': ['qdrant-client[fastembed]>=1.6.4,<2.0.0'],
 'redshift': ['psycopg2-binary>=2.9.1'],
 'redshift:platform_python_implementation == "PyPy"': ['psycopg2cffi>=2.9.0'],
 's3': ['s3fs>=2022.4.0', 'botocore>=1.28'],
 'snowflake': ['snowflake-connector-python>=3.5.0'],
 'synapse': ['pyarrow>=12.0.0', 'adlfs>=2022.4.0', 'pyodbc>=4.0.39,<5.0.0'],
 'weaviate': ['weaviate-client>=3.22']}

entry_points = \
{'console_scripts': ['dlt = dlt.cli._dlt:_main']}

setup_kwargs = {
    'name': 'dlt',
    'version': '0.4.5a0',
    'description': 'dlt is an open-source python-first scalable data loading library that does not require any backend to run.',
    'long_description': '<h1 align="center">\n    <strong>data load tool (dlt) â€” the open-source Python library for data loading</strong>\n</h1>\n<p align="center">\nBe it a Google Colab notebook, AWS Lambda function, an Airflow DAG, your local laptop,<br/>or a GPT-4 assisted development playgroundâ€”<strong>dlt</strong> can be dropped in anywhere.\n</p>\n\n\n<h3 align="center">\n\nðŸš€ Join our thriving community of likeminded developers and build the future together!\n\n</h3>\n\n<div align="center">\n  <a target="_blank" href="https://dlthub.com/community" style="background:none">\n    <img src="https://img.shields.io/badge/slack-join-dlt.svg?labelColor=191937&color=6F6FF7&logo=slack" style="width: 260px;"  />\n  </a>\n</div>\n<div align="center">\n  <a target="_blank" href="https://pypi.org/project/dlt/" style="background:none">\n    <img src="https://img.shields.io/pypi/v/dlt?labelColor=191937&color=6F6FF7">\n  </a>\n  <a target="_blank" href="https://pypi.org/project/dlt/" style="background:none">\n    <img src="https://img.shields.io/pypi/pyversions/dlt?labelColor=191937&color=6F6FF7">\n  </a>\n</div>\n\n## Installation\n\ndlt supports Python 3.8+.\n\n```bash\npip install dlt\n```\n\n## Quick Start\n\nLoad chess game data from chess.com API and save it in DuckDB:\n\n```python\nimport dlt\nfrom dlt.sources.helpers import requests\n# Create a dlt pipeline that will load\n# chess player data to the DuckDB destination\npipeline = dlt.pipeline(\n    pipeline_name=\'chess_pipeline\',\n    destination=\'duckdb\',\n    dataset_name=\'player_data\'\n)\n# Grab some player data from Chess.com API\ndata = []\nfor player in [\'magnuscarlsen\', \'rpragchess\']:\n    response = requests.get(f\'https://api.chess.com/pub/player/{player}\')\n    response.raise_for_status()\n    data.append(response.json())\n# Extract, normalize, and load the data\npipeline.run(data, table_name=\'player\')\n```\n\n\nTry it out in our **[Colab Demo](https://colab.research.google.com/drive/1NfSB1DpwbbHX9_t5vlalBTf13utwpMGx?usp=sharing)**\n\n## Features\n\n- **Automatic Schema:** Data structure inspection and schema creation for the destination.\n- **Data Normalization:** Consistent and verified data before loading.\n- **Seamless Integration:** Colab, AWS Lambda, Airflow, and local environments.\n- **Scalable:** Adapts to growing data needs in production.\n- **Easy Maintenance:** Clear data pipeline structure for updates.\n- **Rapid Exploration:** Quickly explore and gain insights from new data sources.\n- **Versatile Usage:** Suitable for ad-hoc exploration to advanced loading infrastructures.\n- **Start in Seconds with CLI:** Powerful CLI for managing, deploying and inspecting local pipelines.\n- **Incremental Loading:** Load only new or changed data and avoid loading old records again.\n- **Open Source:** Free and Apache 2.0 Licensed.\n\n## Ready to use Sources and Destinations\n\nExplore ready to use sources (e.g. Google Sheets) in the [Verified Sources docs](https://dlthub.com/docs/dlt-ecosystem/verified-sources) and supported destinations (e.g. DuckDB) in the [Destinations docs](https://dlthub.com/docs/dlt-ecosystem/destinations).\n\n## Documentation\n\nFor detailed usage and configuration, please refer to the [official documentation](https://dlthub.com/docs).\n\n## Examples\n\nYou can find examples for various use cases in the [examples](docs/examples) folder.\n\n## Adding as dependency\n\n`dlt` follows the semantic versioning with the [`MAJOR.MINOR.PATCH`](https://peps.python.org/pep-0440/#semantic-versioning) pattern. Currently, we are using **pre-release versioning** with the major version being 0.\n\n- `minor` version change means breaking changes\n- `patch` version change means new features that should be backward compatible\n- any suffix change, e.g., `post10` -> `post11`, is considered a patch\n\nWe suggest that you allow only `patch` level updates automatically:\n* Using the [Compatible Release Specifier](https://packaging.python.org/en/latest/specifications/version-specifiers/#compatible-release). For example **dlt~=0.3.10** allows only versions **>=0.3.10** and less than **<0.4**\n* Poetry [caret requirements](https://python-poetry.org/docs/dependency-specification/). For example **^0.3.10** allows only versions **>=0.3.10** to **<0.4**\n## Get Involved\n\nThe dlt project is quickly growing, and we\'re excited to have you join our community! Here\'s how you can get involved:\n\n- **Connect with the Community**: Join other dlt users and contributors on our [Slack](https://dlthub.com/community)\n- **Report issues and suggest features**: Please use the [GitHub Issues](https://github.com/dlt-hub/dlt/issues) to report bugs or suggest new features. Before creating a new issue, make sure to search the tracker for possible duplicates and add a comment if you find one.\n- **Track progress of our work and our plans**: Please check out our [public Github project](https://github.com/orgs/dlt-hub/projects/9)\n- **Contribute Verified Sources**: Contribute your custom sources to the [dlt-hub/verified-sources](https://github.com/dlt-hub/verified-sources) to help other folks in handling their data tasks.\n- **Contribute code**: Check out our [contributing guidelines](CONTRIBUTING.md) for information on how to make a pull request.\n- **Improve documentation**: Help us enhance the dlt documentation.\n\n## License\n\nDLT is released under the [Apache 2.0 License](LICENSE.txt).\n',
    'author': 'dltHub Inc.',
    'author_email': 'services@dlthub.com',
    'maintainer': 'Marcin Rudolf',
    'maintainer_email': 'marcin@dlthub.com',
    'url': 'https://github.com/dlt-hub',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'extras_require': extras_require,
    'entry_points': entry_points,
    'python_requires': '>=3.8.1,<3.13',
}


setup(**setup_kwargs)
